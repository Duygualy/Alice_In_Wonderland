{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71BSGJcrh1UR",
        "outputId": "967c93fb-0a83-4b04-dc74-5567e5b15608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# We're gonna make a model that's capable of predicting the next character in a sequence for this first we're gonna\n",
        "# give some sequence as an input and it's simply predict the most likely next character. We give romeo and juliet as train inputs\n",
        "# It will predict to us what the most likely next character for that sequence is and then take the output from the model and feed\n",
        "#it as the input again to the model and keep predicting sequence of characters. So it will always keep predicting the next character\n",
        "#from the previous output as many times as we want to generate an entire play\n",
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now we're dowloand the file of data set\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('alice_in_wonderland.txt', 'https://www.gutenberg.org/files/11/11-0.txt')"
      ],
      "metadata": {
        "id": "M70eKt-4kQLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In here we're open our file wirh rb mode so it means read bytes mode then we read that as entire string.\n",
        "#And with this decode(encoding='utf-8') we're gonna turn the byte string array to a normal string\n",
        "text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
        "#We're gonna check the length of the characters in this text so how many characters we have in the text\n",
        "print('Length of the text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXfpWbo4m126",
        "outputId": "8c86a9e7-eed1-41f1-edc8-e1e5fd7118bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the text: 148139 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We're gonna check the first 300 characters in the text\n",
        "print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29cy5iQ-qBI9",
        "outputId": "e53e8ce1-fe0d-4598-d3c3-d8e677467216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿﻿*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN\r\n",
            "WONDERLAND ***\r\n",
            "[Illustration]\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alice’s Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I.     Down the Rabbit-Hole\r\n",
            " CHAPTER II.    The Pool of Tears\r\n",
            " CHAPTER III.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As you can see it's a text data set so we have to encode it as a integer.\n",
        "# We're gonna encode each unique character as a different integer.\n",
        "\n",
        "#With this code we will learn how many unique character in our vocabulary. This will sort every unique character in text\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "#Now we're creating a mapping from unique characters to indices\n",
        "#It's going letter to index\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "# Then we turn the vocabulary as an array so we can use the index at which a letter appears as the reverse mapping\n",
        "#It going indext to letter\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "#with this function we get some text and converts is an int\n",
        "#every single character (c) in our text turn their int representation\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n"
      ],
      "metadata": {
        "id": "hTRioJmzqUtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can see the result of what we're do in the above\n",
        "print(\"Text: \", text[:15])\n",
        "print(\"Encoded Text: \", text_to_int(text[:14]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqZY_YT-ue71",
        "outputId": "431c4334-b9d3-4ffd-ceb4-cbae6a816a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  ﻿﻿*** START OF \n",
            "Encoded Text:  [77 77  7  7  7  2 34 35 16 33 35  2 30 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #We're gonna passing different objects in here so if it's not already a numpy array we turn it as a numpy array because we have to do it for the code work\n",
        "\n",
        "text_as_int = text_to_int(text)\n",
        "\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:14]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGTaDzmzvJK3",
        "outputId": "2116bc7b-6fc2-4c0d-d39d-72bc8c22279b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿﻿*** START OF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#It's very hard to train our model with 92409243097593873 sentence so we're gonna distribute our text datas to short arrays then we give them as train examples\n",
        "#Every train example wil take the seq_length  longth character array as an input then it will output the one-letter shifted version of this string.\n",
        "\n",
        "#length of sequence for a training example\n",
        "seq_length = 300\n",
        "#In here we're calculate we can how many training example. But why 101?\n",
        "#Because in every training example we have to take 100 input and the last training output's last character so +1\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "#It convert our entire data set into character\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
      ],
      "metadata": {
        "id": "z2p_P3GG2l55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It means it gets our entire dataset with char_dataset and then batch it into length 101 and with drop remainder lets say we get 105 caharacter  it will automatically drop the last 4 character\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder= True)"
      ],
      "metadata": {
        "id": "edC-HsCr6Bjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It takes sequences and distribute it as input and target with this we will get the training examples that we need\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "#We use mapping for applying the above function to the every entry and it will be stored insdide the dataset object\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "nDVKn11p7VZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We make it 2 times\n",
        "for x, y in dataset.take(2):\n",
        "  print(\"!!!THE INPUT!!!\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\n\\nTHE OUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL1Hm43a7vM9",
        "outputId": "b8df4ce2-522c-4857-9444-5afffa6a7cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!THE INPUT!!!\n",
            "﻿﻿*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN\r\n",
            "WONDERLAND ***\r\n",
            "[Illustration]\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alice’s Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I.     Down the Rabbit-Hole\r\n",
            " CHAPTER II.    The Pool of Tears\r\n",
            " CHAPTER III.  \n",
            "\n",
            "\n",
            "THE OUTPUT\n",
            "﻿*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN\r\n",
            "WONDERLAND ***\r\n",
            "[Illustration]\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alice’s Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I.     Down the Rabbit-Hole\r\n",
            " CHAPTER II.    The Pool of Tears\r\n",
            " CHAPTER III.   \n",
            "!!!THE INPUT!!!\n",
            "A Caucus-Race and a Long Tale\r\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\r\n",
            " CHAPTER V.     Advice from a Caterpillar\r\n",
            " CHAPTER VI.    Pig and Pepper\r\n",
            " CHAPTER VII.   A Mad Tea-Party\r\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\r\n",
            " CHAPTER IX.    The Mock Turtle’s Story\r\n",
            " CHAPTER X.     The Lobst\n",
            "\n",
            "\n",
            "THE OUTPUT\n",
            " Caucus-Race and a Long Tale\r\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\r\n",
            " CHAPTER V.     Advice from a Caterpillar\r\n",
            " CHAPTER VI.    Pig and Pepper\r\n",
            " CHAPTER VII.   A Mad Tea-Party\r\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\r\n",
            " CHAPTER IX.    The Mock Turtle’s Story\r\n",
            " CHAPTER X.     The Lobste\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will make training batches.\n",
        "\n",
        "#We're gonna feed our model 64 batches of data at a time\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE =len(vocab)\n",
        "#We're gonna define the embedding dimension as the how we want every single vector to represent our words in the embedding layer\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS =1024\n",
        "\n",
        " # its the size to shuffle the dataset\n",
        "BUFFER_SIZE =10000\n",
        "\n",
        "#We will shuffle all the data then with batch , batch it into that size (64) and if there is more use drop remainder\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n"
      ],
      "metadata": {
        "id": "ol3IiwLPBjqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      #None means we don't know how long the sequences are going to be in each batch we just know we're going to have 64 entries in each batch\n",
        "      #but we don't know this 64 entries's long. We don't know how long the sequence is going to be so we leave this one\n",
        "\n",
        "      #You can think of this layer like a translator. It converts each character into a vector that represents it in a meaningful way. In this way, the model uses these vectors when working with text.\n",
        "      # vocab_size is the number of characters the model can recognize. For example, letter a, space, punctuation marks\n",
        "      # embedding_dim is the size of the vector in which each character will be represented. This is necessary so that characters can be represented in a meaningful way.\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                # This gives information about the shape of the input data. batch_size is the number of data groups to be given to the model in each operation. None indicates that the length of each array can be variable.\n",
        "                                batch_input_shape = [batch_size,None]),\n",
        "\n",
        "\n",
        "      #You can think of this layer like a writer. He remembers what he has written in the past and uses this information when deciding what to write in the future.\n",
        "      #Rnn_units: Number of LSTM cells. This determines how complex and powerful the model can be.\n",
        "      #Return_sequences=True: This parameter ensures that it returns intermediate outputs at each time step. In this way, we can understand what the model sees at each step.\n",
        "      #Stateful=True: This allows the model to carry the previous cell state to the next cell. Thus, relations in long sequences are preserved.\n",
        "      #Recurrent_initializer='glorot_uniform': This sets the initial value of the weights of the LSTM.\n",
        "      tf.keras.layers.LSTM(rnn_units,\n",
        "                           #return sequences means return the intermediate stage at every step because we want to look at what the model seeing at the intermediate steps not just the final step\n",
        "                           return_sequences = True,\n",
        "                           stateful = True,\n",
        "                           recurrent_initializer = 'glorot_uniform'),\n",
        "      # In dense layer which is going to contain the amount of vocabulary size nodes. The reasen we'Re doing this is because we want the final layer to have the amount of nodes in it rqual to the amount of characters in the vocabulary.\n",
        "      #with this every nodes can represent a probability distribution the dot character comes next\n",
        "\n",
        "\n",
        "      #You can think of this layer as a estimator. It predicts which character will come after each word written by the author and gives the probability distribution.\n",
        "      #Vocab_size: The number of neurons in the output layer, that is, the size of the vocabulary. Each neuron represents the probability of a particular character appearing.\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "#Now we're building the model with calling build_model function\n",
        "model = build_model(VOCAB_SIZE , EMBEDDING_DIM , RNN_UNITS , BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGGmB5XYCBzv",
        "outputId": "8dc32986-2fdb-4b82-d363-768a8c3a6ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           19968     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 78)            79950     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5346894 (20.40 MB)\n",
            "Trainable params: 5346894 (20.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Before calculating the loss function let's look at an input and output from our untrained model\n",
        "\n",
        "#data.take(1): This takes a batch from the dataset. data here represents your training data. The take(1) statement indicates that we will take only one batch from this dataset.\n",
        "#input_example_batch: The sample data we will give as input to our model.\n",
        "#target_example_batch: Target outputs that our model needs to learn.\n",
        "#This loop assigns a batch of data to the input_example_batch and target_example_batch variables.\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  # model(input_example_batch):ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  #example_batch_predictions: Predictions given by the model. These predictions may initially be random because they are from the model that has not yet been trained.\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape\n",
        "\n",
        "#It gives 78 the reason of this is when we create a dense layer as our last layer it has 78 nodes so every prediction contain 78 numbers\n",
        "#And that's going to be the probability of every one of those characters occurring"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJSlq61IROWW",
        "outputId": "46ef4ad1-74cf-4a67-beb9-95846b4b4161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 300, 78) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can see that the prediction is an array of 78 arrays, one for each entry in the batch\n",
        "#As you can see we get 78 different predictions because we have 78 elements in the batch\n",
        "\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyxibiFSUzOa",
        "outputId": "f31388f3-d074-489c-b46d-75aa49015b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-1.27260350e-02  7.26364646e-03 -8.42981227e-03 ... -2.20729271e-04\n",
            "   -2.05158582e-03  1.36162143e-03]\n",
            "  [-9.98254213e-03  4.79651615e-03 -7.48366676e-03 ... -1.04447955e-03\n",
            "   -5.76748187e-03 -6.76101248e-04]\n",
            "  [-1.20214811e-02  3.17729171e-03 -1.18758641e-02 ...  1.94440479e-03\n",
            "   -7.51129910e-03 -2.83305999e-03]\n",
            "  ...\n",
            "  [-1.48498053e-02  2.38533341e-03 -1.25077944e-02 ... -5.53831458e-04\n",
            "    5.31782024e-03 -3.81807168e-03]\n",
            "  [-6.10411353e-03 -3.20379680e-04 -1.02074463e-02 ... -5.33176586e-04\n",
            "    5.37356222e-03 -1.89389405e-03]\n",
            "  [ 1.04208663e-03 -5.78439096e-04 -7.30897021e-03 ... -2.13013031e-04\n",
            "    4.69692238e-03 -1.37889758e-04]]\n",
            "\n",
            " [[-1.87387019e-02  9.49239917e-03  1.63768069e-03 ...  2.15098169e-03\n",
            "   -1.70066929e-03  1.37356180e-03]\n",
            "  [-1.06666498e-02  6.33637421e-03  1.07635884e-03 ...  4.91729937e-04\n",
            "    7.03625847e-04  2.49040662e-03]\n",
            "  [-9.18274466e-03  8.14232789e-03  1.03456446e-03 ... -3.35736573e-03\n",
            "   -3.25542246e-03  7.20131677e-03]\n",
            "  ...\n",
            "  [-1.14258397e-02 -1.06821209e-03 -1.55118331e-02 ...  5.00963069e-03\n",
            "   -4.03273292e-03 -1.00707402e-02]\n",
            "  [-1.46241616e-02  1.77908828e-03 -1.73623525e-02 ... -4.67571151e-03\n",
            "   -2.73646321e-03 -1.21547980e-02]\n",
            "  [-1.68238021e-02  3.86730069e-03 -1.85059085e-02 ... -1.17642153e-02\n",
            "   -1.93868810e-03 -1.27763934e-02]]\n",
            "\n",
            " [[ 5.99348918e-04  2.01006420e-03 -8.80020962e-04 ... -6.60028309e-05\n",
            "   -8.27514287e-03 -1.47555489e-03]\n",
            "  [-1.72958733e-03 -1.46123068e-03  7.64615834e-03 ... -4.74984292e-04\n",
            "   -1.28981443e-02 -6.38705678e-05]\n",
            "  [-7.22990080e-04 -2.04114779e-03  7.54820555e-03 ... -5.80873340e-03\n",
            "   -5.31413779e-03  5.27101336e-03]\n",
            "  ...\n",
            "  [-1.69502746e-03 -1.55708776e-03 -1.80699141e-03 ...  2.23067869e-03\n",
            "   -1.61569170e-03  5.21757873e-04]\n",
            "  [-4.30054730e-03  1.16376730e-03 -5.49083948e-03 ...  2.17452273e-03\n",
            "   -5.09810983e-04 -2.86045531e-03]\n",
            "  [-9.34541039e-03  5.35982568e-03 -1.06865764e-02 ... -6.82882825e-03\n",
            "    1.02089625e-03 -6.60154689e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.29473312e-02  2.48918752e-03 -7.72181898e-03 ... -3.08183976e-03\n",
            "   -8.44484952e-04 -2.11983128e-03]\n",
            "  [-1.48030883e-02  7.83000840e-04 -1.24345478e-02 ... -6.42263331e-05\n",
            "   -2.90493108e-03 -4.16612253e-03]\n",
            "  [-1.89076141e-02  5.04099531e-03 -7.39305513e-03 ...  3.64928553e-03\n",
            "    1.40906009e-03 -3.68291582e-03]\n",
            "  ...\n",
            "  [-1.39725879e-02  6.73303008e-03 -5.15780644e-03 ... -3.15789878e-03\n",
            "   -3.38888523e-04 -1.85838668e-03]\n",
            "  [-1.48579758e-02  4.54879925e-03  3.88462842e-03 ... -2.33517401e-03\n",
            "   -7.49960029e-03 -6.42803847e-04]\n",
            "  [-1.60374399e-02  2.88015860e-03 -4.04767878e-03 ... -1.98841793e-04\n",
            "   -7.44117284e-03 -2.74291355e-03]]\n",
            "\n",
            " [[-6.35908730e-03 -1.57173141e-03  1.25199147e-02 ...  5.05343545e-04\n",
            "   -1.14091625e-02 -7.84651609e-04]\n",
            "  [-8.72583129e-03  1.07350096e-03  6.50800345e-03 ...  8.82553868e-04\n",
            "   -1.16313640e-02 -5.24507277e-03]\n",
            "  [-1.22112241e-02  5.52846398e-03  7.28403870e-03 ... -9.75561794e-04\n",
            "   -5.79526694e-03 -2.39178003e-03]\n",
            "  ...\n",
            "  [-1.49705186e-02  5.65123558e-03 -1.14611527e-02 ... -2.01889616e-03\n",
            "    3.67003540e-03 -9.71314730e-05]\n",
            "  [-1.58882439e-02  3.29898042e-03 -1.50367676e-03 ... -8.78195628e-04\n",
            "   -3.33249988e-03  1.15731242e-03]\n",
            "  [-1.71101503e-02  1.60575227e-03 -8.45575798e-03 ...  1.38654932e-03\n",
            "   -3.59365693e-03 -1.09148375e-03]]\n",
            "\n",
            " [[-2.37683184e-03  8.00457317e-04 -2.05850322e-03 ... -5.90147742e-04\n",
            "   -1.02948658e-02 -2.42949696e-03]\n",
            "  [ 2.33852305e-04  4.26678127e-03 -3.71807604e-03 ... -2.45044066e-04\n",
            "   -1.60175916e-02 -4.71169502e-03]\n",
            "  [ 3.82411061e-04  2.97965342e-03  1.00246933e-03 ...  3.02509125e-03\n",
            "   -1.18926456e-02 -1.85448583e-03]\n",
            "  ...\n",
            "  [-1.82041340e-02  1.03737144e-02 -2.25963406e-02 ...  4.78976266e-03\n",
            "   -1.48670154e-03 -6.51306566e-03]\n",
            "  [-1.82370897e-02  1.03078578e-02 -2.35434417e-02 ...  4.75385226e-03\n",
            "   -2.79565901e-03 -6.83316216e-03]\n",
            "  [-1.82626639e-02  1.02501735e-02 -2.42561661e-02 ...  4.76282276e-03\n",
            "   -3.85875767e-03 -7.10446760e-03]]], shape=(64, 300, 78), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's look at the first prediction of the first element in the batch\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 300, where each interior array is the prediction for the next character at each time step\n",
        "#In this it will show the predictions of the 300 time steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t038NCxVeB-",
        "outputId": "2cf0ca55-d1ba-487f-c6c5-27a4f83a09d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n",
            "tf.Tensor(\n",
            "[[-0.01272603  0.00726365 -0.00842981 ... -0.00022073 -0.00205159\n",
            "   0.00136162]\n",
            " [-0.00998254  0.00479652 -0.00748367 ... -0.00104448 -0.00576748\n",
            "  -0.0006761 ]\n",
            " [-0.01202148  0.00317729 -0.01187586 ...  0.0019444  -0.0075113\n",
            "  -0.00283306]\n",
            " ...\n",
            " [-0.01484981  0.00238533 -0.01250779 ... -0.00055383  0.00531782\n",
            "  -0.00381807]\n",
            " [-0.00610411 -0.00032038 -0.01020745 ... -0.00053318  0.00537356\n",
            "  -0.00189389]\n",
            " [ 0.00104209 -0.00057844 -0.00730897 ... -0.00021301  0.00469692\n",
            "  -0.00013789]], shape=(300, 78), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's look at the first time step for the first element in the batch\n",
        "\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "\n",
        "#It will give us probability of every single characters occuring next at the first time step. And of course there is 78 values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCOQkE3XWVz_",
        "outputId": "b599d18b-6c3b-4ea4-8d95-ba8069767a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "tf.Tensor(\n",
            "[-1.27260350e-02  7.26364646e-03 -8.42981227e-03 -1.41266715e-02\n",
            "  2.56846496e-03  1.54364761e-02  3.29441857e-03  1.91028696e-03\n",
            " -7.64808850e-03  4.53224313e-03 -2.96300394e-03 -2.75725778e-03\n",
            " -1.09919393e-02 -5.04809991e-03  1.12738106e-02 -1.11805955e-02\n",
            " -7.99321290e-03 -3.71907139e-04  4.08116262e-04  3.19566438e-03\n",
            "  1.30655589e-02 -7.02787098e-03  6.49509486e-03 -1.08425654e-02\n",
            " -6.73757773e-03  5.61409397e-03  1.72738917e-03 -1.00233676e-02\n",
            " -9.26385401e-05 -9.65325627e-03  5.97952306e-03 -2.18841783e-03\n",
            "  6.21370785e-03  6.96362415e-03  9.07788984e-03 -4.57827281e-03\n",
            "  6.01531286e-03  8.08725413e-03 -2.62106350e-03  7.50992820e-03\n",
            " -1.11967651e-03 -7.11710611e-03 -1.06015918e-03 -3.27578397e-03\n",
            " -3.36000603e-03 -1.00620883e-02 -1.60992797e-03  1.46036176e-03\n",
            "  7.37965573e-04 -6.72496296e-03  1.50577128e-02 -2.62487144e-03\n",
            " -6.25204528e-03  7.61709176e-04  2.44490546e-03 -2.83480808e-03\n",
            "  9.00028273e-03 -1.17240092e-02 -3.16363433e-03 -7.20179360e-03\n",
            "  2.92779412e-03 -9.98361618e-04  3.28332465e-03  6.81089517e-03\n",
            "  4.06693015e-03  1.88262924e-03 -7.47770909e-03 -6.05205679e-03\n",
            " -8.31045583e-03  3.34378914e-03 -5.24299336e-04 -9.99332871e-03\n",
            " -1.56215811e-03 -1.41210156e-04  3.71156586e-03 -2.20729271e-04\n",
            " -2.05158582e-03  1.36162143e-03], shape=(78,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If we want to determine the predicted character (from the above array) we need to sample the output distribution (pick a value based on probabilities)\n",
        "#This function samples from the probability distribution on pred. num_samples=1 specifies that one sample will be taken from each probability distribution. This is done to determine which character is the most likely.\n",
        "#This contains indices of samples from pred. Its shape is (batch_size, sequence_length, 1)\n",
        "sampled_indicies = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "#np.reshape(sampled_indicies, (1, -1)): Reshapes the sampled_indicies tensor into (1, -1). This puts all indexes on a single line.\n",
        "#[0]: Gets the first (and only) row from the reshaped tensor. This ensures that the sampled indexes are made into a flat list.\n",
        "sampled_indices = np.reshape(sampled_indicies, (1, -1))[0]\n",
        "\n",
        "#This function converts indexes to characters. Using the previously defined int_to_text function, the corresponding character of each index is found and they are concatenated. This creates the string of characters that the model predicts.\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "#predicted_chars: Contains the sequence of characters the model predicts.You can see them in the under\n",
        "predicted_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QvkvUsZcXDZp",
        "outputId": "86dead47-edc8-4dd0-e09e-3a84a1a9479d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"X’—Qn\\ufeffsyWU3!W\\rYakPs_CYw]M'JlayyHnpogA3JE*AVcW3LcLET—'SkBtE\\rT[xMV’ZS\\r’p!CEg's:x; E]_wdhHjY*XR‘‘[3v[Pw0thqjS,f)tbYIl;]EUDw’YvUTYr.L)mALksqz’;Pdf-‘dIfE\\r\\n*RW‘”bGCQ[vo kJN-3shwrAJdFFS:(?hdE?Ym(,x',\\rjzmQ‘ù?E\\ufeff’g.h[’Dgd;rl'jjvcJXW“cU3EùB‘EXtfw!if\\nA‘jOTDAK :dobjCC”qRV0o]wE”S!)J“S?VeyL’(\\rpBlc])-3\\nPZk_LII0FFdW\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#So we create a loss function that can compare the output to the expected output and give us some numeric value representing how close the two were\n",
        "def loss(labels,logits):\n",
        "  #it takes all the labels and all of the probability distributions(logits) and will compute a loss on those. So we can learn how different or how similar those two things are\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
      ],
      "metadata": {
        "id": "wbrcyoiJbxHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we're finally compile the model with adam optimizer and for loss function we will use the loss that we creae for loss function\n",
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "metadata": {
        "id": "o1nQwBfBeE_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we are going to setup and configure our model to save checkpoinst as it trains. This will allow us to load our model from a checkpoint and continue training it.\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "#checkpoint_prefix: Name and directory path of checkpoint files. The os.path.join function joins the string checkpoint_dir and ckpt_{epoch}. {epoch} is replaced with the current epoch number during training, thus creating a separate checkpoint file for each epoch.\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "#tf.keras.callbacks.ModelCheckpoint: This callback is used to save the weights of the model at certain intervals.\n",
        "#filepath: The path to save checkpoint files. checkpoint_prefix is ​​used here.\n",
        "#save_weights_only=True: Saves only the weights of the model, not the full architecture of the model. This provides faster speed and smaller file sizes.\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "#This configuration saves checkpoints at regular intervals (at the end of each epoch) during training of the model. These checkpoints contain the weights of the model so that they can be restarted if training of the model is interrupted."
      ],
      "metadata": {
        "id": "CGB5T2EfrXw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we're training our model\n",
        "history = model.fit(data, epochs = 40, callbacks = [checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw9pedZdrnc-",
        "outputId": "7c5522ac-e337-47db-f938-e06f2b68ed23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "7/7 [==============================] - 204s 28s/step - loss: 3.7492\n",
            "Epoch 2/40\n",
            "7/7 [==============================] - 241s 35s/step - loss: 3.2412\n",
            "Epoch 3/40\n",
            "7/7 [==============================] - 215s 31s/step - loss: 3.1796\n",
            "Epoch 4/40\n",
            "7/7 [==============================] - 216s 31s/step - loss: 3.1403\n",
            "Epoch 5/40\n",
            "7/7 [==============================] - 192s 27s/step - loss: 3.0801\n",
            "Epoch 6/40\n",
            "7/7 [==============================] - 189s 27s/step - loss: 2.9987\n",
            "Epoch 7/40\n",
            "7/7 [==============================] - 195s 28s/step - loss: 2.8823\n",
            "Epoch 8/40\n",
            "7/7 [==============================] - 200s 28s/step - loss: 2.7246\n",
            "Epoch 9/40\n",
            "7/7 [==============================] - 210s 30s/step - loss: 2.5795\n",
            "Epoch 10/40\n",
            "7/7 [==============================] - 234s 34s/step - loss: 2.4915\n",
            "Epoch 11/40\n",
            "7/7 [==============================] - 235s 34s/step - loss: 2.4170\n",
            "Epoch 12/40\n",
            "7/7 [==============================] - 216s 32s/step - loss: 2.3607\n",
            "Epoch 13/40\n",
            "7/7 [==============================] - 197s 28s/step - loss: 2.2969\n",
            "Epoch 14/40\n",
            "7/7 [==============================] - 191s 27s/step - loss: 2.2711\n",
            "Epoch 15/40\n",
            "7/7 [==============================] - 193s 28s/step - loss: 2.2240\n",
            "Epoch 16/40\n",
            "7/7 [==============================] - 196s 27s/step - loss: 2.1807\n",
            "Epoch 17/40\n",
            "7/7 [==============================] - 193s 27s/step - loss: 2.1393\n",
            "Epoch 18/40\n",
            "7/7 [==============================] - 190s 27s/step - loss: 2.1011\n",
            "Epoch 19/40\n",
            "7/7 [==============================] - 197s 28s/step - loss: 2.0632\n",
            "Epoch 20/40\n",
            "7/7 [==============================] - 196s 28s/step - loss: 2.0230\n",
            "Epoch 21/40\n",
            "7/7 [==============================] - 189s 27s/step - loss: 1.9874\n",
            "Epoch 22/40\n",
            "7/7 [==============================] - 189s 27s/step - loss: 1.9548\n",
            "Epoch 23/40\n",
            "7/7 [==============================] - 190s 27s/step - loss: 1.9209\n",
            "Epoch 24/40\n",
            "7/7 [==============================] - 190s 27s/step - loss: 1.8839\n",
            "Epoch 25/40\n",
            "7/7 [==============================] - 194s 28s/step - loss: 1.8564\n",
            "Epoch 26/40\n",
            "7/7 [==============================] - 195s 28s/step - loss: 1.8232\n",
            "Epoch 27/40\n",
            "7/7 [==============================] - 189s 27s/step - loss: 1.7944\n",
            "Epoch 28/40\n",
            "7/7 [==============================] - 190s 27s/step - loss: 1.7663\n",
            "Epoch 29/40\n",
            "7/7 [==============================] - 194s 27s/step - loss: 1.7432\n",
            "Epoch 30/40\n",
            "7/7 [==============================] - 192s 27s/step - loss: 1.7071\n",
            "Epoch 31/40\n",
            "7/7 [==============================] - 199s 29s/step - loss: 1.6825\n",
            "Epoch 32/40\n",
            "7/7 [==============================] - 184s 26s/step - loss: 1.6576\n",
            "Epoch 33/40\n",
            "7/7 [==============================] - 189s 27s/step - loss: 1.6367\n",
            "Epoch 34/40\n",
            "7/7 [==============================] - 196s 28s/step - loss: 1.6079\n",
            "Epoch 35/40\n",
            "7/7 [==============================] - 185s 26s/step - loss: 1.5767\n",
            "Epoch 36/40\n",
            "7/7 [==============================] - 186s 26s/step - loss: 1.5555\n",
            "Epoch 37/40\n",
            "7/7 [==============================] - 191s 28s/step - loss: 1.5356\n",
            "Epoch 38/40\n",
            "7/7 [==============================] - 194s 28s/step - loss: 1.5074\n",
            "Epoch 39/40\n",
            "7/7 [==============================] - 196s 28s/step - loss: 1.4846\n",
            "Epoch 40/40\n",
            "7/7 [==============================] - 189s 27s/step - loss: 1.4609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#After training our model we need to rebuild our model with using a new batch size of one. This means it only runs on one instance at a time.\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN UNITS, batch_size = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "9UcnmcpFse0D",
        "outputId": "7e221ce3-a10c-4ea5-eba6-77724463a53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-1-505df83635e0>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-505df83635e0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN UNITS, batch_size = 1)\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In the training our model code block we're gonna do 40 checkpoints and every checkpoint is like checkpoint at epoch 1 or checkpoint at epoch 2 ...\n",
        "#To get the latest checkpoint we're doing this\n",
        "model.load_weights(tf.train.lates_checkpoint(checkpoint_dir)\n",
        "#1 means we expect the input as 1 and None means we don't know what the next dimension length will be\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "fvNYKxHntOmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint_num: Specifies the number of the checkpoint to be loaded. In this example, we want to load checkpoint 10.\n",
        "checkpoint_num = 10\n",
        "\n",
        "#tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)): Loads the checkpoint in the specified file path. Loads weights from file ./training_checkpoints/ckpt_10.\n",
        "#model.load_weights(...): Applies the loaded weights to the model.\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "\n",
        "#model.build(tf.TensorShape([1, None])): Rebuilds the model. tf.TensorShape([1, None]) indicates that the input shape of the model is (1, None). This means the batch size is 1 and the array length is indeterminate (variable).\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "ZlBmyNEauv6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate is 1000\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  #we need to preprocess this text again so it works properly\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.So we just write it randomly and not necessarly need this code too\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  #We rebuild our model but it will keep the old data so wee need to clear\n",
        "  model.reset_states()\n",
        "  #it will generate 1000 characters\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      #it's take our predictions and removes exterior dimension so we just have the predictions we want not extra dimensions\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "       #turn our integers into a text\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "#and then we retuen everything\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "jucIvYoXzyXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user = input(\"Type a starting string: \")\n",
        "print(generate_text(model, user))"
      ],
      "metadata": {
        "id": "Q6ThUzXJ1m0g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}